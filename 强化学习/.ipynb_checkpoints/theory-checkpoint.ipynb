{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概述\n",
    "\n",
    "## 目的\n",
    "> 找到能使长期累计奖赏最大化的策略 $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 方法汇总\n",
    "\n",
    "## Model-free  vs  Model-based \n",
    "> model 表示用模型来表示环境\n",
    "- model-free：  Q learning, Sarsa, Policy Gradients 都是从环境中得到反馈然后从中学习 \n",
    "\n",
    "## Policy-based  vs  Value-based  \n",
    "> 基于概率和基于价值\n",
    "- 基于概率是强化学习中最直接的一种, 他能通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动, 所以每种动作都有可能被选中, 只是可能性不同. 而基于价值的方法输出则是所有动作的价值, 我们会根据最高价值来选着动作。\n",
    "- 基于概率：Policy Gradients  基于价值： Q learning, Sarsa  结合：Actor-Critic\n",
    "\n",
    "## Monte-Carlo update  vs  Temporal-Difference update\n",
    "> 回合更新和单步更新\n",
    "-  Monte-carlo learning 和基础版的 policy gradients 等 都是回合更新制, Qlearning, Sarsa, 升级版的 policy gradients 等都是单步更新制.\n",
    "\n",
    "## On-Policy  vs  Off-Policy\n",
    "> 在线学习和离线学习。 在线学习, 就是指我必须本人在场, 并且一定是本人边玩边学习。离线学习 同样是从过往的经验中学习, 但是这些过往的经历没必要是自己的经历, 任何人的经历都能被学习. 或者我也不必要边玩边学习,\n",
    "-  在线学习： Sarsa， 还有一种优化 Sarsa 的算法, 叫做 Sarsa lambda, 最典型的离线学习就是 Q learning, Deep-Q-Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理论模型\n",
    "\n",
    "## K-摇臂赌博机(K-armed bandit)\n",
    "> 对应单步更新强化学习，最大化单步奖赏\n",
    "\n",
    "### 仅探索(exploration-only)\n",
    "> 获知每个摇臂的期望奖赏\n",
    "\n",
    "### 仅利用(exploitation-only)\n",
    "> 执行奖赏最大的动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
